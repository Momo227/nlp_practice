{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter4_2_4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM86S1nMMVpBgLgSNBVe/VN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Momo227/nlp_practice/blob/Chapter4/Chapter4/Chapter4_2_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA8MT2k7BSqA"
      },
      "source": [
        "**ストップワード**：自然言語処理をする際に、一般的で役に立たないなどの理由で処理対象外にする単語のこと。\n",
        "\n",
        "例えば…\n",
        "    助詞や助動詞の機能語（「は」 「の」 「です」 「ます」 など）\n",
        " * 出現頻度が高い割に役に立たない\n",
        " * 計算量や性能に悪影響を及ぼす\n",
        "\n",
        "方式\n",
        "* 辞書による方式\n",
        "* 出現頻度による方式"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df6mKJR-GBxV"
      },
      "source": [
        "**辞書による方式**：予めスワップワードを辞書に定義しておき、辞書内に含まれる単語をテキストから除去する。\n",
        "\n",
        "素朴で簡単だが、欠点も。。\n",
        "* 辞書を作るためのコストが高い。\n",
        "* あるデータセットで有効な辞書が別のデータセットでは役に立たない→対象としているデータセットによって辞書を作り替える必要がある"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF9fLMHDICMl"
      },
      "source": [
        "今回すること\n",
        "1.   ダウンロードしたデータを読みこむ\n",
        "2.   open関数を用いてファイルを開き、一定ずつ読み込んでいく\n",
        "3.   stripメソッドを使って記号を取り除く\n",
        "4.   setメソッドを使ってリストを集合に変換する（リストより集合の方が、検索が早いため）\n",
        "5.   読み込めたらストップワードを除去する関数を定義する\n",
        "6.   Janomeで形態素解析をしてテキストを単語に分割し、単語列とスワップファートを与える関数（5. ）から単語列の中でスワップワードが含まれているものがあれば除去する。\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKGaNs4c2CXP",
        "outputId": "1c659622-bf73-4a23-931b-b29bb5290f45"
      },
      "source": [
        "pip install janome"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting janome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/63/98858cbead27df7536c7e300c169da0999e9704d02220dc6700b804eeff0/Janome-0.4.1-py2.py3-none-any.whl (19.7MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7MB 1.3MB/s \n",
            "\u001b[?25hInstalling collected packages: janome\n",
            "Successfully installed janome-0.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg3ETqu6BAU0"
      },
      "source": [
        "with open('Japanese.txt', 'r', encoding='utf-8') as f:\n",
        "    stopwords = [w.strip() for w in f]\n",
        "    stopwords = set(stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOtTBfd51Fpr"
      },
      "source": [
        "def remove_stopwords(words, stopwords):\n",
        "    words = [w for w in words if w not in stopwords]\n",
        "    return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Yhiu9pH1b6M"
      },
      "source": [
        "from janome.tokenizer import Tokenizer\n",
        "t = Tokenizer(wakati=True)\n",
        "text = 'りんごをいくつか買う。'\n",
        "words = t.tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kq46nYIk2SdV",
        "outputId": "314473e5-87b1-4634-9c9c-668be1b85ad2"
      },
      "source": [
        "words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Tokenizer.__tokenize_stream at 0x7f4bb0cad4d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H30yc1iL2UCG",
        "outputId": "65cef6b8-7274-4281-9c72-d6c56f5385e1"
      },
      "source": [
        "remove_stopwords(words, stopwords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['りんご', 'を', 'か', '買う', '。']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yuga34ht2WfA"
      },
      "source": [
        "**出現頻度による方式**：リスト内の単語を頻度をカウントし、高頻度(低頻度)の単語をテキストから除去する\n",
        "\n",
        "・高頻度の単語を除去する時→それらの単語テスト内で占める割合が高い一方、役に立たない時\n",
        "\n",
        "（ex. \"of\", \"the\", …）"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ely8R7_c5Qwj"
      },
      "source": [
        "今回の作業\n",
        "1.   open関数を用いてファイルを開き、readで読み込む。\n",
        "2.   コーパスは形態素解析済みで、各単語は空白で区切られているので、Pythonのsplitメソッドを使って単語分割をする。\n",
        "3.   各単語の出現頻度をカウントする。（Python組み込みのCounterクラスを使う）\n",
        "      \n",
        "      →リストを与えて初期化することで、各要素の頻度をカウントする。\n",
        "4.   得られた上位n件の単語スワップワードとみなし、先程定義した、remove_stopwordsを使ってストップを除去する。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bErTHOCg25f-"
      },
      "source": [
        "with open('ja.text8', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "    words = text.split()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgm7gpFW6qn-",
        "outputId": "679f6c03-0816-4e62-b4bc-2ee1ac683fbd"
      },
      "source": [
        "from collections import Counter\n",
        "Counter(['cat', 'dog', 'cat'])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'cat': 2, 'dog': 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxvAhE3o6_ne",
        "outputId": "b7a8676e-c22c-4a9a-8dd6-8b89563a3124"
      },
      "source": [
        "fdist = Counter(words)\n",
        "fdist.most_common(n=10)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('の', 34758),\n",
              " ('、', 33214),\n",
              " ('。', 22662),\n",
              " ('に', 21765),\n",
              " ('は', 20387),\n",
              " ('を', 18005),\n",
              " ('た', 17003),\n",
              " ('が', 14859),\n",
              " ('で', 14634),\n",
              " ('て', 10976)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    }
  ]
}